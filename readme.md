<h1 align="center">ğŸš€ Retail AWS Data Warehouse Pipeline ğŸš€</h1>

## ğŸ“Œ Tá»•ng quan

Dá»± Ã¡n nÃ y lÃ  má»™t há»‡ thá»‘ng **Data Warehouse cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng**, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u chuá»—i cung á»©ng bÃ¡n láº». Há»‡ thá»‘ng sá»­ dá»¥ng cÃ¡c dá»‹ch vá»¥ **AWS** vÃ  cÃ¡c cÃ´ng cá»¥ **Data Engineering hiá»‡n Ä‘áº¡i** nháº±m xÃ¢y dá»±ng má»™t **háº¡ táº§ng dá»¯ liá»‡u máº¡nh máº½**, há»— trá»£ doanh nghiá»‡p Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh dá»±a trÃªn dá»¯ liá»‡u.

## ğŸ” Má»¥c tiÃªu & á»¨ng dá»¥ng thá»±c táº¿  
Há»‡ thá»‘ng nÃ y giÃºp doanh nghiá»‡p **tá»‘i Æ°u hoáº¡t Ä‘á»™ng kinh doanh** thÃ´ng qua cÃ¡c phÃ¢n tÃ­ch chuyÃªn sÃ¢u (*insight*), bao gá»“m:

- **ğŸ“Š Hiá»‡u suáº¥t bÃ¡n hÃ ng** â€“ XÃ¡c Ä‘á»‹nh sáº£n pháº©m bÃ¡n cháº¡y, xu hÆ°á»›ng theo mÃ¹a vÃ  sá»± tÄƒng trÆ°á»Ÿng doanh thu.
- **ğŸ“¦ Tá»‘i Æ°u tá»“n kho** â€“ PhÃ¢n tÃ­ch cung cáº§u Ä‘á»ƒ trÃ¡nh tÃ¬nh tráº¡ng thiáº¿u hÃ ng hoáº·c dÆ° thá»«a hÃ ng hÃ³a.
- **ğŸ›’ HÃ nh vi khÃ¡ch hÃ ng** â€“ PhÃ¢n khÃºc khÃ¡ch hÃ ng dá»±a trÃªn thÃ³i quen mua sáº¯m Ä‘á»ƒ cÃ¡ nhÃ¢n hÃ³a chiáº¿n lÆ°á»£c marketing.
- **âš™ï¸ Hiá»‡u suáº¥t váº­n hÃ nh** â€“ PhÃ¡t hiá»‡n cÃ¡c nÃºt tháº¯t trong chuá»—i cung á»©ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a logistics vÃ  thá»i gian giao hÃ ng.

## ğŸ¯ Objectives
- **Centralized Data Warehouse**: Store structured data in **Amazon Redshift**.
- **Efficient ETL Process**: Use **PySpark** and **Airflow** to process and load data.
- **Scalable Storage**: Store raw & staging data in **AWS S3**.
- **Interactive Analytics**: Create business intelligence dashboards with **Power BI**.
- **Containerized Deployment**: Use **Docker** for easy orchestration.

---

## ğŸ—ï¸ Architecture Diagram
## Data Pipeline Diagram
![Alt text](data/image/pipeline.PNG)

[Click here to view the diagram](https://viewer.diagrams.net/?tags=%7B%7D&lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&dark=auto#G1z

## ğŸ› ï¸ Tech Stack

| Tool/Service        | Purpose                                |
|---------------------|----------------------------------------|
| **Amazon Redshift** | Data Warehouse                         |
| **AWS S3**         | Storage for raw & staging data         |
| **Apache Airflow**  | Orchestration for ETL workflows       |
| **PySpark**        | Data processing & transformations      |
| **Power BI**       | Business Intelligence & visualization  |
| **Docker**         | Containerized deployment              |

---

## ğŸ”„ Data Pipeline Workflow

1. **Ingestion**: Raw data is downloaded from [Kaggle Dataset](https://www.kaggle.com/datasets/alinoranianesfahani/dataco-smart-supply-chain-for-big-data-analysis) and stored in **AWS S3**.
2. **Processing (ETL)**:
   - PySpark processes the data, performs transformations, and loads it into **staging S3**.
3. **Loading**: Transformed data is loaded into **Amazon Redshift**.
4. **Analysis**: Power BI connects to Redshift for interactive dashboards.
5. **Automation**: Airflow schedules and monitors the pipeline.

---

## ğŸ“¦ Setup & Deployment

### Prerequisites
- AWS account (S3, Redshift setup)
- Docker & Docker Compose
- Airflow installed
- PySpark installed

### Steps to Run

1. **Clone the Repository**
   ```bash
   git clone https://github.com/Duong27102001/retail_aws_pipeline.git
   cd retail_aws_pipeline
