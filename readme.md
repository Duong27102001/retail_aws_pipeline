<h1 align="center">ğŸš€ Retail AWS Data Warehouse Pipeline ğŸš€</h1>

## ğŸ“Œ Tá»•ng quan

Dá»± Ã¡n nÃ y lÃ  má»™t há»‡ thá»‘ng **Data Warehouse cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng**, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u chuá»—i cung á»©ng bÃ¡n láº». Há»‡ thá»‘ng sá»­ dá»¥ng cÃ¡c dá»‹ch vá»¥ **AWS** vÃ  cÃ¡c cÃ´ng cá»¥ **Data Engineering hiá»‡n Ä‘áº¡i** nháº±m xÃ¢y dá»±ng má»™t **háº¡ táº§ng dá»¯ liá»‡u máº¡nh máº½**, há»— trá»£ doanh nghiá»‡p Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh dá»±a trÃªn dá»¯ liá»‡u.

Há»‡ thá»‘ng nÃ y giÃºp doanh nghiá»‡p **tá»‘i Æ°u hoáº¡t Ä‘á»™ng kinh doanh** thÃ´ng qua cÃ¡c phÃ¢n tÃ­ch chuyÃªn sÃ¢u (*insight*), bao gá»“m:

- **ğŸ“Š PhÃ¢n tÃ­ch quÃ¡ trÃ¬nh Ä‘áº·t hÃ ng vÃ  giao hÃ ng** â€“ PhÃ¢n tÃ­ch thá»i gian giao hÃ ng vÃ  phÆ°Æ¡ng thá»©c váº­n chuyá»ƒn Ä‘á»ƒ tá»‘i Æ°u hÃ³a quy trÃ¬nh giao hÃ ng vÃ  giáº£m thiá»ƒu cÃ¡c trÆ°á»ng há»£p giao hÃ ng muá»™n.
- **ğŸ“¦ PhÃ¢n tÃ­ch doanh thu bÃ¡n hÃ ng** â€“ PhÃ¢n tÃ­ch cung cáº§u Ä‘á»ƒ trÃ¡nh tÃ¬nh tráº¡ng thiáº¿u hÃ ng hoáº·c dÆ° thá»«a hÃ ng hÃ³a.
- **ğŸ›’ PhÃ¢n tÃ­ch hÃ nh vi khÃ¡ch hÃ ng vÃ  phÆ°Æ¡ng thá»©c thanh toÃ¡n** â€“ PhÃ¢n khÃºc khÃ¡ch hÃ ng dá»±a trÃªn thÃ³i quen mua sáº¯m Ä‘á»ƒ cÃ¡ nhÃ¢n hÃ³a chiáº¿n lÆ°á»£c marketing.
- **âš™ï¸ Hiá»‡u suáº¥t váº­n hÃ nh** â€“ PhÃ¡t hiá»‡n cÃ¡c nÃºt tháº¯t trong chuá»—i cung á»©ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a logistics vÃ  thá»i gian giao hÃ ng.
---
## ğŸ¯ Má»¥c tiÃªu vÃ  cÃ´ng nghá»‡ sá»­ dá»¥ng
- **Centralized Data Warehouse**: Store structured data in **Amazon Redshift**.
- **Efficient ETL Process**: Use **PySpark** and **Airflow** to process and load data.
- **Scalable Storage**: Store raw & staging data in **AWS S3**.
- **Interactive Analytics**: Create business intelligence dashboards with **Power BI**.
- **Containerized Deployment**: Use **Docker** for easy orchestration.

### ğŸ—ï¸ Architecture Diagram
![Alt text](data/image/pipeline.PNG)

### ğŸ”„ Data Pipeline Workflow

1. **Ingestion**: Raw data is downloaded from [Kaggle Dataset](https://www.kaggle.com/datasets/alinoranianesfahani/dataco-smart-supply-chain-for-big-data-analysis) and stored in **AWS S3**.
2. **Processing (ETL)**:
   - PySpark processes the data, performs transformations, and loads it into **staging S3**.
3. **Loading**: Transformed data is loaded into **Amazon Redshift**.
4. **Analysis**: Power BI connects to Redshift for interactive dashboards.
5. **Automation**: Airflow schedules and monitors the pipeline.
---

## ğŸ“¦ Setup & Deployment

### Prerequisites
- AWS account (S3, Redshift setup)
- Docker & Docker Compose
- Airflow installed
- PySpark installed

### Steps to Run

1. **Clone the Repository**
   ```bash
   git clone https://github.com/Duong27102001/retail_aws_pipeline.git
   cd retail_aws_pipeline
