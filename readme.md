# ğŸš€ Retail AWS Data Warehouse Pipeline

## ğŸ“Œ Overview
This project is a **scalable data warehouse pipeline** designed for a **retail supply chain dataset**. The architecture leverages **AWS services** and **modern data engineering tools** to process, store, and visualize data for business insights.

## ğŸ¯ Objectives
- **Centralized Data Warehouse**: Store structured data in **Amazon Redshift**.
- **Efficient ETL Process**: Use **PySpark** and **Airflow** to process and load data.
- **Scalable Storage**: Store raw & staging data in **AWS S3**.
- **Interactive Analytics**: Create business intelligence dashboards with **Power BI**.
- **Containerized Deployment**: Use **Docker** for easy orchestration.

---

## ğŸ—ï¸ Architecture Diagram

       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Kaggle Dataset â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   AWS S3 (Raw) â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ PySpark (ETL Jobs) â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ AWS S3 (Staging)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Amazon Redshift    â”‚  <-- Data Warehouse
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Power BI          â”‚  <-- Data Visualization
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## ğŸ› ï¸ Tech Stack

| Tool/Service        | Purpose                                |
|---------------------|----------------------------------------|
| **Amazon Redshift** | Data Warehouse                         |
| **AWS S3**         | Storage for raw & staging data         |
| **Apache Airflow**  | Orchestration for ETL workflows       |
| **PySpark**        | Data processing & transformations      |
| **Power BI**       | Business Intelligence & visualization  |
| **Docker**         | Containerized deployment              |

---

## ğŸ”„ Data Pipeline Workflow

1. **Ingestion**: Raw data is downloaded from [Kaggle Dataset](https://www.kaggle.com/datasets/alinoranianesfahani/dataco-smart-supply-chain-for-big-data-analysis) and stored in **AWS S3**.
2. **Processing (ETL)**:
   - PySpark processes the data, performs transformations, and loads it into **staging S3**.
3. **Loading**: Transformed data is loaded into **Amazon Redshift**.
4. **Analysis**: Power BI connects to Redshift for interactive dashboards.
5. **Automation**: Airflow schedules and monitors the pipeline.

---

## ğŸ“¦ Setup & Deployment

### Prerequisites
- AWS account (S3, Redshift setup)
- Docker & Docker Compose
- Airflow installed
- PySpark installed

### Steps to Run

1. **Clone the Repository**
   ```bash
   git clone https://github.com/Duong27102001/retail_aws_pipeline.git
   cd retail_aws_pipeline
